{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UltimateGuitarTabs Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook scrapes the top 1000 most popular tabs posting on https://www.ultimate-guitar.com/\n",
    "\n",
    "After scraping, it stores the data found into a MySQL database ('UltimateGuitarTabs') with the following tables:\n",
    "1. Tab_Data (id, song, artist, is_acoustic, tab_url)\n",
    "    - id: Unique tab ID \n",
    "    - song: Name of song\n",
    "    - artist: Name of artist\n",
    "    - is_acoustic: Song is played acoustically (1), otherwise (0)\n",
    "    - tab_url: URL to tab\n",
    "2. Artists (name, url)\n",
    "    - name: Name of artist\n",
    "    - url: URL to artist profile\n",
    "3. Hits (id, num_hits, votes, rating)\n",
    "    - id: Unique tab ID\n",
    "    - num_hits: # of times tab visited\n",
    "    - votes: # number of votes received\n",
    "    - rating: Avg rating out of 5\n",
    "4. Chords (song, artist, tonality, capo, chords)\n",
    "    - song: Name of song\n",
    "    - artist: Name of artist\n",
    "    - tonality: Original of song\n",
    "    - capo: Fret to place capo\n",
    "    - chords: String of full chord progression\n",
    "    \n",
    "Scraped on June 5, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import time\n",
    "from lxml.html import fromstring\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:20]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "\n",
    "#If you are copy pasting proxy ips, put in the list below\n",
    "#proxies = ['121.129.127.209:80', '124.41.215.238:45169', '185.93.3.123:8080', '194.182.64.67:3128', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
    "proxies = get_proxies()\n",
    "proxy_pool = cycle(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_database(db=\"UltimateGuitarTabs\"):\n",
    "    \"\"\"\n",
    "    Connects to MySQL database and \n",
    "    returns the connection\"\"\"\n",
    "    db = pymysql.connect(host=\"localhost\",  # your host \n",
    "                         user=\"root\",       # username\n",
    "                         passwd=\"\",     # password\n",
    "                         db=db)   # name of the database\n",
    "    return db\n",
    "\n",
    "#proxy = next(proxy_pool)\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Source: https://realpython.com/python-web-scraping-practical-introduction/\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #with closing(get(url, stream=True, proxies={\"http\": proxy, \"https\": proxy})) as resp:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                print(resp.status_code)\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Source: https://realpython.com/python-web-scraping-practical-introduction/\n",
    "    Returns true if the response seems to be HTML, false otherwise\n",
    "    \"\"\"\n",
    "    content_type = None\n",
    "    if 'Content-Type' in resp.headers:\n",
    "        content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    Source: https://realpython.com/python-web-scraping-practical-introduction/\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)\n",
    "\n",
    "def get_data(url):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    global proxy\n",
    "    html = ''\n",
    "    soup = ''\n",
    "    while html == '' or soup == '':\n",
    "        try:\n",
    "            html = simple_get(url)\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            #Data is stored as JSON on the page   \n",
    "            script = soup.find('script', text=re.compile('window.UGAPP.store.page'))\n",
    "            # Removes unnecessary text\n",
    "            json_text = re.search(r'^\\s*window\\.UGAPP\\.store\\.page\\s*=\\s*({.*?})\\s*;\\s*$', \\\n",
    "                              script.string, flags=re.DOTALL | re.MULTILINE).group(1)\n",
    "            data = json.loads(json_text)\n",
    "            return(data)\n",
    "        except Exception as e:\n",
    "            print('Exception: '+str(e))\n",
    "            print(url)\n",
    "            #proxy = next(proxy_pool)\n",
    "            #print('new proxy:',proxy)\n",
    "\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "def get_tabs_data(url):\n",
    "    \"\"\"\n",
    "    Creates the bs4 object and extracts a list\n",
    "    of tab info. Hits info is stored as a separate \n",
    "    list in the html file so it is returned separately.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = get_data(url)\n",
    "    tab_links = data['data']['data']['tabs']       \n",
    "    hits = data['data']['data']['hits']\n",
    "    return(tab_links, hits)\n",
    "\n",
    "def get_chords(url):\n",
    "    \"\"\"\n",
    "    Scrapes and returns the sequences of \n",
    "    chords as a list as well as the fret number\n",
    "    to place a capo. \n",
    "    \"\"\"\n",
    "\n",
    "    data = get_data(url)\n",
    "\n",
    "    chords = data['data']['tab_view']['wiki_tab']['content']\n",
    "    \n",
    "    # Matching groups (open tag)(chord pitch)(base note {0 or 1})(chord type)(base note {0 or 1})(closing tag)\n",
    "    pattern = \"(\\[ch\\])([A-G]+)(\\/[A-G]*[b#])*([(?m)|(?m\\d)|(?b\\d)|(?#\\d)|(?maj\\d)|(?add\\d)|(?sus\\d)|(?aug)|(?aug\\d)|(?dim)|(?dim\\d)]*)(\\/[A-G]*[b#])*(\\[\\/ch\\])\"\n",
    "    prog = re.compile(pattern)\n",
    "    result = prog.findall(chords)\n",
    "    \n",
    "    cleaned_res = result\n",
    "    for i in range(len(result)):\n",
    "        # Grabbing groups (chord pitch)(base note)(chord type)(base note)\n",
    "        cleaned_res[i] = result[i][1] + result[i][2] + result[i][3] + result[i][4]\n",
    "       \n",
    "    # Grabbing Capo info\n",
    "    capo = 0\n",
    "    try:\n",
    "        capo = data['data']['tab_view']['meta']['capo']\n",
    "    except:\n",
    "        capo = 0\n",
    "\n",
    "    return(cleaned_res, capo)\n",
    "    \n",
    "def get_genre(url):\n",
    "    \"\"\"\n",
    "    Grabs the artist's categorized genre\n",
    "    \"\"\"\n",
    "    data = get_data(url)\n",
    "    genre = data['data']['artist']['genre']\n",
    "    \n",
    "    return(genre)\n",
    "\n",
    "def add_to_db(tab_list, hits, recreate=False):\n",
    "    \"\"\"\n",
    "    Creates and adds data to four MySQL tables\n",
    "    \"\"\"\n",
    "    db = connect_to_database()\n",
    "    cur = db.cursor()\n",
    "    \n",
    "    if recreate:\n",
    "        delete_tabs = \"DROP TABLE IF EXISTS Tab_Data;\"\n",
    "        cur.execute(delete_tabs)\n",
    "        delete_artists = \"DROP TABLE IF EXISTS Artists;\"\n",
    "        cur.execute(delete_artists)\n",
    "        delete_hits = \"DROP TABLE IF EXISTS Hits;\"\n",
    "        cur.execute(delete_hits)\n",
    "        delete_chords = \"DROP TABLE IF EXISTS Chords;\"\n",
    "        cur.execute(delete_chords)\n",
    "\n",
    "\n",
    "        create_tabs = \"CREATE TABLE Tab_Data (id INT(11) NOT NULL PRIMARY KEY, song CHAR(50), artist CHAR(50), \\\n",
    "                                    is_acoustic INT(1), tab_url TEXT(500));\"\n",
    "        cur.execute(create_tabs)\n",
    "\n",
    "        create_artists = \"CREATE TABLE Artists (name CHAR(50) NOT NULL PRIMARY KEY, url TEXT(500));\"\n",
    "        cur.execute(create_artists)\n",
    "\n",
    "\n",
    "        create_hits = \"CREATE TABLE Hits (id INT(11) NOT NULL PRIMARY KEY, num_hits INT(8), \\\n",
    "                       votes INT(11), rating FLOAT);\"\n",
    "        cur.execute(create_hits)\n",
    "\n",
    "        create_chords = \"CREATE TABLE Chords (id INT(11) NOT NULL PRIMARY KEY, song CHAR(50), artist CHAR(50), tonality CHAR(3), capo INT(2), chords TEXT(500));\"\n",
    "        cur.execute(create_chords)\n",
    "        seen_id = set()\n",
    "    else:\n",
    "        cur.execute(\"SELECT Tab_Data.id FROM Tab_Data JOIN Chords on Tab_Data.id = Chords.id Join Hits on Tab_Data.id = Hits.id\")\n",
    "        seen_id = set(id[0] for id in cur.fetchall())\n",
    "            \n",
    "    for i in range(len(tab_list)):\n",
    "        if (i+1)%50==0:\n",
    "            print(i+1)\n",
    "        song = tab_list[i]    \n",
    "        hit = hits[i]\n",
    "\n",
    "        tab_id = int(song['id'])\n",
    "        if tab_id in seen_id:\n",
    "            continue\n",
    "        seen_id.add(tab_id)\n",
    "        song_name = song['song_name'][:50].replace(\"'\",\"\\\\'\")\n",
    "        artist = song['artist_name'][:50].replace(\"'\",\"\\\\'\")\n",
    "        tonality = song['tonality_name']\n",
    "        votes = int(song['votes'])\n",
    "        rating = float(song['rating'])\n",
    "        is_acoustic = int(song['recording']['is_acoustic'])\n",
    "        tab_url = song['tab_url']\n",
    "        artist_url = song['artist_url']\n",
    "        hit_id = int(hit['id'])\n",
    "        hit_num = int(hit['hits'])\n",
    "        chords, capo = get_chords(song['tab_url'])\n",
    "        \n",
    "        sql_tab = \"INSERT INTO Tab_Data (id,song,artist,is_acoustic,tab_url) \\\n",
    "        VALUES ('%d','%s','%s','%d','%s')\" % \\\n",
    "        (tab_id, song_name, artist, is_acoustic, tab_url)\n",
    "\n",
    "        sql_artist = \"INSERT INTO Artists (name, url) VALUES ('%s','%s')\" %(artist, artist_url)\n",
    "        \n",
    "        sql_hit = \"INSERT INTO Hits (id, num_hits, votes, rating) VALUES ('%d','%d','%d','%.8f')\" % (tab_id, hit_num, votes, rating)\n",
    "\n",
    "        sql_chords = \"INSERT INTO Chords (id, song, artist, tonality, capo, chords) VALUES('%d','%s','%s','%s','%d','%s')\" % \\\n",
    "        (tab_id, song_name, artist, tonality, int(capo), ','.join(chords))\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            cur.execute(sql_tab)\n",
    "            db.commit()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            db.rollback()\n",
    "        \n",
    "        try:\n",
    "            cur.execute(sql_artist)\n",
    "            db.commit()\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            db.rollback()\n",
    "            \n",
    "        try:\n",
    "            cur.execute(sql_hit)\n",
    "            db.commit()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            db.rollback()\n",
    "\n",
    "        try:\n",
    "            cur.execute(sql_chords)\n",
    "            db.commit()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            db.rollback()\n",
    "            \n",
    "    # disconnect from server\n",
    "    db.close()\n",
    "    print('done')\n",
    "    return True\n",
    "\n",
    "def get_multiple_pages(url, genres, years, n):\n",
    "    \"\"\"\n",
    "    Creates functionality to scrape multiple\n",
    "    pages up to n\n",
    "    \"\"\"\n",
    "    page_suffix = \"&page=\"\n",
    "    year_suffix = \"&decade[]=\"\n",
    "    genre_suffix = \"&genres[]=\"\n",
    "    tabs_list = []\n",
    "    hits_list = []\n",
    "    \n",
    "    for genre in genres:\n",
    "        for year in range(years[0], years[1]+1, 10):\n",
    "            for i in range(n):\n",
    "                print(genre,year,i+1)\n",
    "                cur_tabs, cur_hits = get_tabs_data(url + genre_suffix + str(genre) + year_suffix + str(year) + page_suffix + str(i))\n",
    "                tabs_list += cur_tabs\n",
    "                hits_list += cur_hits\n",
    "\n",
    "    print('found %d urls'%len(tabs_list))\n",
    "    return(tabs_list, hits_list)\n",
    "   \n",
    "def print_table(table, count=False):\n",
    "    \"\"\"\n",
    "    Prints database table\n",
    "    \"\"\"\n",
    "    db = connect_to_database()\n",
    "    cur = db.cursor()\n",
    "\n",
    "    # Select data from table using SQL query.\n",
    "    if count:\n",
    "        cur.execute(\"SELECT COUNT(*) FROM \"+table)\n",
    "        cnt = cur.fetchall()[0][0]\n",
    "    else:    \n",
    "        cnt = cur.execute(\"SELECT * FROM \"+table)\n",
    "        for row in cur.fetchall() :\n",
    "            print(row)\n",
    "    print(table, cnt)\n",
    "    return(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tabs, hits = get_multiple_pages(\"https://www.ultimate-guitar.com/explore?order=hitstotal_desc&type[]=Chords\", [4,14,49,666], [1960,2010], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_db(tabs, hits, recreate=False)\n",
    "print_table('tab_data',count=True)\n",
    "print_table('chords', count=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
